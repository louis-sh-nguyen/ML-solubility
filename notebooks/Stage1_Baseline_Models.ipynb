{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7180a784",
   "metadata": {},
   "source": [
    "# Stage 1 – Baseline & Classical Models\n",
    "**Polymer–Solvent Solubility Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce73e698",
   "metadata": {},
   "source": [
    "## Notebook Goals  \n",
    "1. Explore & tidy the dataset  \n",
    "2. Train a variety of classical scikit‑learn regressors **(Linear → XGBoost)**, explaining the intuition behind each.  \n",
    "3. Fit a vanilla PyTorch MLP and compare results.  \n",
    "4. Record metrics for later stages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4566d7a8",
   "metadata": {},
   "source": [
    "### 0 · Environment & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa58fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "\"\"\"Install missing deps (Colab)\"\"\"\n",
    "# !pip install --quiet pandas numpy scikit-learn xgboost torch torchvision torchaudio matplotlib seaborn tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc015093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math, pathlib\n",
    "import numpy as np; import pandas as pd\n",
    "import matplotlib.pyplot as plt; import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED); random.seed(RANDOM_SEED); torch.manual_seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b921a2f",
   "metadata": {},
   "source": [
    "### 1 · Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9491985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('..') / 'data'  # adjust\n",
    "df_exp     = pd.read_csv(DATA_DIR / 'experimental_dataset.csv')\n",
    "df_poly    = pd.read_csv(DATA_DIR / 'list_of_polymers.csv')\n",
    "df_solv    = pd.read_csv(DATA_DIR / 'list_of_solvents.csv')\n",
    "df_poly_mw = pd.read_csv(DATA_DIR / 'polymer_mass.csv')\n",
    "df_solv_mw = pd.read_csv(DATA_DIR / 'solvent_mass.csv')\n",
    "df_solv_mac= pd.read_csv(DATA_DIR / 'solvent_macro_features.csv')\n",
    "\n",
    "print('experimental_dataset:', df_exp.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2abb313",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = (df_exp\n",
    "           .merge(df_poly,    on='polymer_id',  how='left')\n",
    "           .merge(df_solv,    on='solvent_id',  how='left')\n",
    "           .merge(df_poly_mw, on='polymer_id',  how='left')\n",
    "           .merge(df_solv_mw, on='solvent_id',  how='left')\n",
    "           .merge(df_solv_mac,on='solvent_id',  how='left'))\n",
    "print(full_df.shape)\n",
    "full_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93014176",
   "metadata": {},
   "source": [
    "### 2 · Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcfe56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COL = 'solubility'  # adjust\n",
    "full_df.isna().sum().sort_values(ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5042067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(full_df[TARGET_COL], kde=True)\n",
    "plt.title('Target distribution'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6dd833",
   "metadata": {},
   "source": [
    "### 3 · Feature Engineering & Pre‑processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb1a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [c for c in full_df.columns if full_df[c].dtype != 'object' and c not in ['polymer_id','solvent_id', TARGET_COL]]\n",
    "cat_cols = [c for c in full_df.columns if full_df[c].dtype == 'object']\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', Pipeline([('scaler', StandardScaler())]), num_cols),\n",
    "    ('cat', Pipeline([('onehot', OneHotEncoder(handle_unknown='ignore'))]), cat_cols)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71832b",
   "metadata": {},
   "source": [
    "### 4 · Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43ee899",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df, test_df = train_test_split(full_df, test_size=0.15, random_state=RANDOM_SEED)\n",
    "train_df, val_df     = train_test_split(train_val_df, test_size=0.15, random_state=RANDOM_SEED)\n",
    "print(len(train_df), len(val_df), len(test_df))\n",
    "\n",
    "X_train, y_train = train_df.drop(columns=[TARGET_COL]), train_df[TARGET_COL]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170d0a51",
   "metadata": {},
   "source": [
    "## 5 · Model Zoo – Train & Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752a55ce",
   "metadata": {},
   "source": [
    "#### 5.1 Linear Regression – baseline, interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aaf55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = Pipeline([('pre', preprocessor), ('lr', LinearRegression())])\n",
    "lr_rmse = -cross_val_score(pipe_lr, X_train, y_train, scoring='neg_root_mean_squared_error', cv=5).mean()\n",
    "print('LinearRegression CV RMSE:', lr_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432c32e1",
   "metadata": {},
   "source": [
    "#### 5.2 Ridge Regression – shrink coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1cb422",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in [0.01,0.1,1,10]:\n",
    "    score = -cross_val_score(Pipeline([('pre', preprocessor), ('ridge', Ridge(alpha=a))]),\n",
    "                              X_train, y_train, scoring='neg_root_mean_squared_error', cv=5).mean()\n",
    "    print(f'alpha={a}: {score:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22596a5",
   "metadata": {},
   "source": [
    "#### 5.3 k‑NN – non‑parametric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd56cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [3,5,10,20]:\n",
    "    score = -cross_val_score(Pipeline([('pre', preprocessor), ('knn', KNeighborsRegressor(n_neighbors=k))]),\n",
    "                              X_train, y_train, scoring='neg_root_mean_squared_error', cv=5).mean()\n",
    "    print(f'k={k}: {score:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331966ef",
   "metadata": {},
   "source": [
    "#### 5.4 Decision Tree – capturing non‑linear splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d768480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for depth in [None,5,10,20]:\n",
    "    score = -cross_val_score(Pipeline([('pre', preprocessor), ('dt', DecisionTreeRegressor(max_depth=depth))]),\n",
    "                              X_train, y_train, scoring='neg_root_mean_squared_error', cv=5).mean()\n",
    "    print(f'depth={depth}: {score:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a0605b",
   "metadata": {},
   "source": [
    "#### 5.5 Random Forest – ensemble reduces variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32534f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = Pipeline([('pre', preprocessor), ('rf', RandomForestRegressor(n_estimators=300, n_jobs=-1, random_state=RANDOM_SEED))])\n",
    "rf_rmse = -cross_val_score(rf, X_train, y_train, scoring='neg_root_mean_squared_error', cv=5).mean()\n",
    "print('RandomForest CV RMSE:', rf_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4420ab4d",
   "metadata": {},
   "source": [
    "#### 5.6 Gradient Boosting – sequential error correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8b6412",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = Pipeline([('pre', preprocessor), ('gb', GradientBoostingRegressor(n_estimators=500, learning_rate=0.05))])\n",
    "gb_rmse = -cross_val_score(gb, X_train, y_train, scoring='neg_root_mean_squared_error', cv=5).mean()\n",
    "print('GradientBoosting CV RMSE:', gb_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c6b323",
   "metadata": {},
   "source": [
    "#### 5.7 Support Vector Regression – kernel trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f1725",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ['linear','rbf']:\n",
    "    svr = SVR(kernel=k, C=5, epsilon=0.1)\n",
    "    score = -cross_val_score(Pipeline([('pre', preprocessor), ('svr', svr)]),\n",
    "                              X_train, y_train, scoring='neg_root_mean_squared_error', cv=5).mean()\n",
    "    print(f'SVR-{k}: {score:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f7c160",
   "metadata": {},
   "source": [
    "#### 5.8 XGBoost – powerful boosting with regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b75aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(n_estimators=800, max_depth=6, learning_rate=0.05, subsample=0.8,\n",
    "                  colsample_bytree=0.8, objective='reg:squarederror', random_state=RANDOM_SEED, n_jobs=-1)\n",
    "pipe_xgb = Pipeline([('pre', preprocessor), ('xgb', xgb)])\n",
    "xgb_rmse = -cross_val_score(pipe_xgb, X_train, y_train, scoring='neg_root_mean_squared_error', cv=5).mean()\n",
    "print('XGBoost CV RMSE:', xgb_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdefbb85",
   "metadata": {},
   "source": [
    "## 6 · Vanilla PyTorch MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d64727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit(X_train)\n",
    "X_train_np = preprocessor.transform(X_train).astype(np.float32)\n",
    "y_train_np = y_train.values.astype(np.float32).reshape(-1,1)\n",
    "\n",
    "class SolvDataset(Dataset):\n",
    "    def __init__(self,X,y): self.X=torch.tensor(X); self.y=torch.tensor(y)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self,idx): return self.X[idx], self.y[idx]\n",
    "\n",
    "dloader = DataLoader(SolvDataset(X_train_np,y_train_np), batch_size=256, shuffle=True)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(in_dim,256), nn.ReLU(), nn.BatchNorm1d(256),\n",
    "                                 nn.Linear(256,128), nn.ReLU(), nn.BatchNorm1d(128),\n",
    "                                 nn.Linear(128,1))\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "model = MLP(X_train_np.shape[1]); opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss();\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train(); total=0\n",
    "    for xb,yb in dloader:\n",
    "        opt.zero_grad(); pred=model(xb); loss=loss_fn(pred,yb); loss.backward(); opt.step(); total+=loss.item()*len(xb)\n",
    "    if (epoch+1)%10==0:\n",
    "        print(f'Epoch {epoch+1:02d}  Train MSE={total/len(dloader.dataset):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a22cfa0",
   "metadata": {},
   "source": [
    "### 7 · Wrap‑up\n",
    "Summarise RMSEs from each model above, draw conclusions, and plan Stage 2."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
